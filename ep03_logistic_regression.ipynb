{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"My full name\"  # write YOUR NAME\n",
    "\n",
    "honorPledge = \"I affirm that I have not given or received any unauthorized \" \\\n",
    "              \"help on this assignment, and that this work is my own.\\n\"\n",
    "\n",
    "\n",
    "print(\"\\nName: \", name)\n",
    "print(\"\\nHonor pledge: \", honorPledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:#FFFFFF;background-color:#2e4053;\">MAC0460 / MAC5832 (2021)</font>\n",
    "<hr>\n",
    "\n",
    "## <font style=\"background-color:#abebc6\">EP3: Logistic regression</font>\n",
    "\n",
    "### Topics / concepts explored in this EP:\n",
    "\n",
    "- Implementation of the **logistic regression algorithm**, using the gradient descent technique\n",
    "- Application on binary classification of 2D examples  (i.e., $d=2$)\n",
    "- Confusion matrix, effects of unbalanced classes\n",
    "\n",
    "Complete and submit this notebook. Places to be filled are indicated with <font style=\"background-color: #f7dc6f\">this color</font>\n",
    "\n",
    "### Evaluation  criteria\n",
    "- Correctitude of the algorithms\n",
    "- Code\n",
    "    - do not change the prototype of the functions\n",
    "    - efficiency (you should avoid unnecessary loops; use matrix/vector computation with NumPy wherever appropriate)\n",
    "    - cleanliness (do not leave any commented code or useless variables)\n",
    "- Appropriateness of the answers\n",
    "- File format: Complete and submit this notebook <font color=\"red\">with the outputs of the execution</font>. **Do no change the file name.**\n",
    "<hr>\n",
    "\n",
    "### Hints\n",
    "- In this notebook vectors are implemented as <tt>ndarray (n,)</tt> and not as <tt>ndarray (n,1)</tt>\n",
    "- It might be wise to first make sure your implementation is correct. For instance, you can compare the results of your implementation with the ones obtained with the implementation available in <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>. After you feel confident, paste your code in the notebook, and then run the rest of the code in the notebook.\n",
    "- If you face difficulties with Keras or other libraries used in this notebook, as well as clarity issues in the exercises in this notebook, post a message in the <a href=\"https://edisciplinas.usp.br/mod/forum/view.php?id=3409630\">Forum for discussions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm\n",
    "\n",
    "We will use the formulation described in the textbook (<i>Learning from data</i>, Abu-Mostafa <i>et al.</i>). Positive class label is equal to 1 and negative class label is equal to -1.\n",
    "\n",
    "The loss (or cost) function to be minimized is\n",
    "$$\n",
    "E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ln(1 + e^{-y^{(i)} \\mathbf{w}^T \\mathbf{x}^{(i)}}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Its gradient is given by\n",
    "\n",
    "$$\\nabla E_{in}(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^{N} \\frac{y^{(i)} \\mathbf{x}^{(i)}}{1 + e^{y^{(i)} \\mathbf{w}^T \\mathbf{x}^{(i)}}}  \\tag{2}$$\n",
    "\n",
    "The logistic (sigmoid) function is\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}  \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training and prediction algorithms\n",
    "\n",
    "<br>\n",
    "In the next four code cells, write the code to implement the specified functions. These functions will be used below for logistic regression training and prediction, in Sections 2 to 4. Use vectorial computation with NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.1. Cross-entropy loss</font>\n",
    "This is Equation (1) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(w, X, y):\n",
    "    \"\"\"\n",
    "    Computes the loss (equation 1)\n",
    "    :param w: weight vector\n",
    "    :type: np.ndarray(shape=(1+d, ))\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: class labels\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return loss: loss (equation 1)\n",
    "    :rtype: float\n",
    "    \"\"\"    \n",
    "    \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function cross_entropy_loss() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.2. Gradient of the cross-entropy loss</font>\n",
    "\n",
    "This is Equation (2) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_gradient(w, X, y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function (equation 2)\n",
    "    :param w: weight vector\n",
    "    :type: np.ndarray(shape=(1+d, ))\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: class labels\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return grad: gradient (equation 2)\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ===> Your code begins here \n",
    "    raise NotImplementedError(\"Function cross_entropy_gradient() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.3 Logistic regression training</font>\n",
    "\n",
    "The function below receives the data matrix  <tt> X (shape = (N, d))</tt> and the ouput vector <tt>y (shape = (N,))</tt>, and should return the final weight vector <tt>w (shape = (d+1,))</tt> and, optionally (when  parameter <tt>return_history = True</tt>), a list of size <tt>num_iterations+1</tt> with the cross-entropy loss values at the beginning and after each of the iterations.\n",
    "\n",
    "Note that the data matrix needs to be extended with a column of 1's.\n",
    "\n",
    "If <tt>w0==None</tt> it must be initialized  with <tt>w0 = np.random.normal(loc = 0, scale = 1, size = X.shape[1])</tt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(X, y, learning_rate = 1e-1, w0 = None,\\\n",
    "                        num_iterations = 1000, return_history = False):\n",
    "    \"\"\"\n",
    "    Computes the weight vector applying the gradient descent technique\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: class label\n",
    "    :type y: np.ndarray(shape=(N, ))\n",
    "    :return: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d, ))\n",
    "    :return: the history of loss values (optional)\n",
    "    :rtype: list of float\n",
    "    \"\"\"    \n",
    "     \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function train_logistic() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">1.4. Logistic regression prediction</font>\n",
    "\n",
    "The function in the next cell will be used to do the prediction of logistic regression. Recall that the prediction is a score in $[0,1]$, given by the sigmoid value of the linear combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def predict_logistic(X, w):\n",
    "    \"\"\"\n",
    "    Computes the logistic regression prediction\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N,d))\n",
    "    :param w: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d,))\n",
    "    :return: predicted classes \n",
    "    :rtype: np.ndarray(shape=(N,))\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ===> Your code begins here\n",
    "    raise NotImplementedError(\"Function predict_logistic() is not implemented\")\n",
    "    ### ===> Your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Generate two blobs of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two blobs\n",
    "N = 300\n",
    "X, y = make_blobs(n_samples=N, centers=2, cluster_std=1, n_features=2, random_state=2)\n",
    "\n",
    "# change labels 0 to -1\n",
    "y[y==0] = -1\n",
    "\n",
    "print(\"X.shape =\", X.shape, \"  y.shape =\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Let's plot the blobs of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "# plot negatives in red\n",
    "plt.scatter(X[y==-1,0], \\\n",
    "            X[y==-1,1], \\\n",
    "            alpha = 0.5,\\\n",
    "            c = 'red')\n",
    "\n",
    "# and positives in blue\n",
    "plt.scatter(x=X[y==1,0], \\\n",
    "            y=X[y==1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'blue')\n",
    "\n",
    "P=+1\n",
    "N=-1\n",
    "legend_elements = [ Line2D([0], [0], marker='o', color='r',\\\n",
    "                    label='Class %d'%N, markerfacecolor='r',\\\n",
    "                    markersize=10),\\\n",
    "                    Line2D([0], [0], marker='o', color='b',\\\n",
    "                    label='Class %d'%P, markerfacecolor='b',\\\n",
    "                    markersize=10) ]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='best')\n",
    "plt.show()      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">2.3. Let's train the linear regressor and plot the loss curve</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(567)\n",
    "\n",
    "# ==> Replace the right hand side below with a call to the\n",
    "# train_logistic() function defined above. Use parameter return_history=True\n",
    "\n",
    "w_logistic, loss = np.array([0,0,0]), [0]\n",
    "\n",
    "# ==> Your code insert ends here\n",
    "\n",
    "print()\n",
    "print(\"Final weight:\\n\", w_logistic)\n",
    "print()\n",
    "print(\"Final loss:\\n\", loss[-1])\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Now, let's plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1min = min(X[:,0])\n",
    "x1max = max(X[:,0])\n",
    "x2min = min(X[:,1])\n",
    "x2max = max(X[:,1])\n",
    "\n",
    "y_pred = predict_logistic(X, w_logistic)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title(\"Ground-truth\")\n",
    "\n",
    "# plot negatives in red\n",
    "ax1.scatter(X[y==-1,0], \\\n",
    "            X[y==-1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'red')\n",
    "\n",
    "# and positives in blue\n",
    "ax1.scatter(x=X[y==1,0], \\\n",
    "            y=X[y==1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'blue')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax2.set_title(\"Prediction\")\n",
    "ax2.scatter(x = X[:,0], y = X[:,1], c = -y_pred, cmap = 'coolwarm')\n",
    "ax2.legend(handles=legend_elements, loc='best')\n",
    "ax2.set_xlim([x1min-1, x1max+1])\n",
    "ax2.set_ylim([x2min-1, x2max+1])\n",
    "\n",
    "p1 = (x1min, -(w_logistic[0] + (x1min)*w_logistic[1])/w_logistic[2])\n",
    "p2 = (x1max, -(w_logistic[0] + (x1max)*w_logistic[1])/w_logistic[2])\n",
    "\n",
    "lines = ax2.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='g', linewidth=4.0)\n",
    "\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing on the data we have collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Read and prepare the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv('QT1data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does filtering out the 'children' make any difference ?\n",
    "# df = df[df['Height']>130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only features of interest\n",
    "feature_cols = ['Height', 'Weight']\n",
    "X = (df.loc[:, feature_cols]).to_numpy(dtype=float)\n",
    "\n",
    "# Input normalization\n",
    "for i in range(X.shape[1]):\n",
    "    avg = np.mean(X[:, i])\n",
    "    stddev = np.std(X[:, i])\n",
    "    X[:, i] = (X[:, i] - avg) / stddev\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variable is Sex\n",
    "sex = df.pop('Sex').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to negative=-1 and positive=1 using functions from scikit-learn\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(sex)\n",
    "y = np.where(y==0, -1, y)\n",
    "print(y.shape)\n",
    "print(min(y), max(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font style=\"background-color: #f7dc6f\">3.2. Training </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> Write the code for \n",
    "#      - training,\n",
    "#      - printing the final weight vector\n",
    "#      - plotting the loss curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">3.3. Ploting the ground-truth, prediction + decision boundary</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> Write the code for\n",
    "#      - computing the prediction\n",
    "#      - plotting the scatterplot with the ground-truth \n",
    "#        and another with the prediction and decision boundary line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"background-color: #f7dc6f\">3.4 How good is the separation?</font>\n",
    "\n",
    "Based on the predicted probabilities, we can decide the final class label for each example $\\mathbf{x}$ as follows:\n",
    "\n",
    "$$\\mbox{class label of $\\mathbf{x}$} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "+1, & \\mbox{if $\\hat{P}(y=1|\\mathbf{x}) > 0.5$,}\\\\\n",
    "-1, & \\mbox{if $\\hat{P}(y=1|\\mathbf{x}) \\leq 0.5$}\\\\\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> write your code to compute how many wrong\n",
    "#      classifications we would have if we use the decision rule above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Repeat for $d>2$ variables\n",
    "\n",
    "In this case, there is no need to display the scatter plot.\n",
    "\n",
    "(Just for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MNIST Dataset \n",
    "\n",
    "This is a well known dataset, commonly used as a first example to illustrate image classification tasks. We could say it is the \"Hello world!\" of image classification. It consists of handwritten digits, divided into $60000$ training images and $10000$ test images. All images are gray-scale (one channel with pixel intensities varying from 0 to 255) and have size $28 \\times 28$. There are 10 classes, corresponding to digits 0 to 9.\n",
    "\n",
    "We could use the $28 \\times 28$ pixel intensities as features. However, here we will perform feature extraction from the images and then the classification based on the extracted features.\n",
    "\n",
    "The dataset is available in many places. Here we will use the one available   with Keras [1]. \n",
    "More information on MNIST can be found at the [oficial site](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "[1]: François Chollet and others, Keras, https://keras.io, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Getting and inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_all, y_train_all), (X_test_all, y_test_all) = mnist.load_data()\n",
    "\n",
    "print(X_train_all.shape, y_train_all.shape)\n",
    "print(X_test_all.shape, y_test_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class distribution of MNIST (training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train_all, return_counts=True)\n",
    "plt.bar(unique, counts, 0.5)\n",
    "plt.title('Class Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">Class distribution of MNIST (test set)</font>\n",
    "Repeat **Class distribution of MNIST** for the testing set (next cell) and compare and comment about the distributions of the training and of the testing sets. Do you think this type of comparison is important? Comment.\n",
    "\n",
    "===> Your comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of some of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize = (9, 6))\n",
    "\n",
    "for i in range(6):\n",
    "    ax[i//3, i%3].imshow(X_train_all[i], cmap='gray')\n",
    "    ax[i//3, i%3].axis('off')\n",
    "    ax[i//3, i%3].set_title(\"Class: %d\"%y_train_all[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Feature extraction\n",
    "\n",
    "Note that the images consist of 28×28=784 pixel values and they could be used as \"raw features\" of the images. However, here we will extract some features from the images and perform classification using them instead of the pixel values.\n",
    "\n",
    "### Mean intensity\n",
    "\n",
    "In the book _Learning from Data_ [2], one of the attributes (features) used by the authors is the mean intensity of the pixel values. This feature is directly related to the proportion of the pixels corresponding to the digit in the image. For instance, it is reasonable to expect that a digit 5 or 2 occupies more pixels than the digit 1 and, therefore, the mean intensity of the first two should be larger than that of the digit 1.\n",
    "\n",
    "### Symmetry\n",
    "\n",
    "The second attribute used by the authors is horizontal symmetry.\n",
    "\n",
    "Symmetry will be defined in terms of asymetry. We define asymmetry as the pixelwise mean of the absolute difference between the pixels values from the original image and those from the corresponding horizotally flipped image. Then, symmetry is defined as the negative of asymmetry.\n",
    "\n",
    "[2]: Yaser S Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from Data, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_intensity(image):\n",
    "    return np.mean(image)\n",
    "\n",
    "def Hsimmetry(image):\n",
    "    # The processing below invert the order of the columns of the image\n",
    "    reflected_image = image[:, ::-1]\n",
    "    return -np.mean(np.abs(image - reflected_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixels $\\rightarrow$ Features ##\n",
    "\n",
    "The above functions for feature extraction will be applied to the samples, both on the training and the test sets. After the feature extraction process below, each image will be represented by two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts an image into a list of features,\n",
    "# using the feature computation functions defined above\n",
    "def convert2features(image):\n",
    "    return np.array([mean_intensity(image),\n",
    "                     Hsimmetry(image)])\n",
    "\n",
    "# feature names\n",
    "F = ['Mean intensity', 'Hsimmetry']\n",
    "\n",
    "# Generate the feature representation for all images\n",
    "X_train_features = np.array([convert2features(image) for image in X_train_all])\n",
    "X_test_features  = np.array([convert2features(image) for image in X_test_all])\n",
    "\n",
    "print(X_train_features.shape)\n",
    "print(X_test_features.shape)\n",
    "\n",
    "for i in range(0,X_train_features.shape[1]):\n",
    "    print()\n",
    "    print(\"Mean of '%s' = %f\" \\\n",
    "          %(F[i],np.mean(X_train_features[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization** of feature values is a common procedure. Here we apply the z-score formula. Note that the normalization parameters (mean and standard deviation) are computed only on training data. To normalize the test data, we use the same parameters.\n",
    "(We suggest you to think why we should not compute the mean and standard deviation over the training+test set. There is no need to answer this here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the scale of feature values; standardize them.\n",
    "# (Yes, the features in the test set should be standardized using\n",
    "#  the statistics of the features in the training set) -- why ??\n",
    "for i in range(X_train_features.shape[1]):\n",
    "    avg = np.mean(X_train_features[:, i])\n",
    "    stddev = np.std(X_train_features[:, i])\n",
    "    X_train_features[:, i] = (X_train_features[:, i] - avg) / stddev\n",
    "    X_test_features[:, i] = (X_test_features[:, i] - avg) / stddev\n",
    "\n",
    "print(\"Shape of X_train: \", X_train_features.shape)\n",
    "print(\"Shape of X_test: \", X_test_features.shape)\n",
    "\n",
    "# print the mean value of each of the features\n",
    "print()\n",
    "print(\"Training set, after normalization:\")\n",
    "for i in range(0,X_train_features.shape[1]):\n",
    "    print(\"  Mean value of '%s' = %f\" \\\n",
    "          %(F[i],np.mean(X_train_features[:, i])))\n",
    "    \n",
    "print()\n",
    "print(\"Testing set, after normalization:\")\n",
    "for i in range(0,X_test_features.shape[1]):\n",
    "    print(\"  Mean value of '%s' = %f\" \\\n",
    "          %(F[i],np.mean(X_test_features[:, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic regression training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Select a subset from two of the classes\n",
    "\n",
    "Here we select two classes as well as a subset of the examples in each class. All code from here on will use the selected subset. You may change later the selected classes and the number of samples in each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let us select two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 5  # positive class\n",
    "N = 1  # negative class\n",
    "\n",
    "X_train_P = X_train_features[y_train_all == P]\n",
    "X_train_N = X_train_features[y_train_all == N]\n",
    "y_train_P = y_train_all[y_train_all == P]\n",
    "y_train_N = y_train_all[y_train_all == N]\n",
    "\n",
    "X_test_P = X_test_features[y_test_all == P]\n",
    "X_test_N = X_test_features[y_test_all == N]\n",
    "y_test_P = y_test_all[y_test_all == P]\n",
    "y_test_N = y_test_all[y_test_all == N]\n",
    "\n",
    "print(\"Positive class: \", X_train_P.shape, y_train_P.shape)\n",
    "print(\"Negative class: \", X_train_N.shape, y_train_N.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">Now we will select a subset of examples from each of the two classes</font>\n",
    "\n",
    "In the following cell, write where it is indicated by <tt>===></tt> the code to change the label of the positive class to +1 and of the negative class to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of positives and negatives to be effectively considered\n",
    "# in the training data to be explored in the remainder of this notebook\n",
    "nP = 100\n",
    "nN = 100\n",
    "\n",
    "X_train = np.concatenate([X_train_P[:nP], X_train_N[:nN]], axis = 0)\n",
    "y_train = np.concatenate([y_train_P[:nP], y_train_N[:nN]], axis = 0).astype('float32')\n",
    "\n",
    "X_test = np.concatenate([X_test_P, X_test_N], axis = 0)\n",
    "y_test = np.concatenate([y_test_P, y_test_N], axis = 0).astype('float32')\n",
    "\n",
    "\n",
    "# ===> Change positive class label to +1 and negative class label to -1\n",
    "\n",
    "\n",
    "### Your code insert ends here\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(56789)\n",
    "def shuffle(X, y):\n",
    "    # input and output must be shuffled equally\n",
    "    perm = np.random.permutation(len(X))\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "X_test, y_test = shuffle(X_test, y_test)\n",
    "\n",
    "print(\"Training X and y --> \", X_train.shape, y_train.shape)\n",
    "print()\n",
    "print(\"Testing X and y --> \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the selected data\n",
    "\n",
    "Let us plot the selected subset of data. Negative examples will be plotted in red and positive ones in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(ax,X,y):\n",
    "    # negatives in red\n",
    "    ax.scatter(X[y==-1,0], \\\n",
    "               X[y==-1,1], \\\n",
    "               label='Class=%d'%N, c = 'red', alpha = 0.5)\n",
    "\n",
    "    # and positives in blue\n",
    "    ax.scatter(x=X[y==1,0], \\\n",
    "               y=X[y==1,1], \\\n",
    "               label='Class=%d'%P, c = 'blue', alpha = 0.5)\n",
    "\n",
    "    ax.set_xlabel(F[0])\n",
    "    ax.set_ylabel(F[1])\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "plot_features(ax,X_train,y_train)\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">4.3.2 Training</font>\n",
    "\n",
    "Run the code in the following cell a few times, each time with different values for  the learning rate and the number of iterations. Comment the behavior of the loss curve. Which values do you consider as good choices?\n",
    "\n",
    "**Note:** for your submission, keep the execution output corresponding to the best parameter values you have found.\n",
    " \n",
    "===> Your comments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(56789)\n",
    "w_logistic, loss = train_logistic(X_train, y_train,\\\n",
    "                                  learning_rate = 0.005,\n",
    "                                  num_iterations = 1000,\\\n",
    "                                  return_history = True)\n",
    "\n",
    "print()\n",
    "print(\"Final weight:\\n\", w_logistic)\n",
    "print()\n",
    "print(\"Final loss:\\n\", loss[-1])\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ploting the scores and decision boundary graphs (training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1min = min(X_train[:,0])\n",
    "x1max = max(X_train[:,0])\n",
    "x2min = min(X_train[:,1])\n",
    "x2max = max(X_train[:,1])\n",
    "\n",
    "y_pred = predict_logistic(X_train, w_logistic)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title(\"Ground-truth\")\n",
    "\n",
    "# plot negatives in red\n",
    "ax1.scatter(X_train[y_train==-1,0], \\\n",
    "            X_train[y_train==-1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'red')\n",
    "\n",
    "# and positives in blue\n",
    "ax1.scatter(x=X_train[y_train==1,0], \\\n",
    "            y=X_train[y_train==1,1], \\\n",
    "            alpha = 0.5, \\\n",
    "            c = 'blue')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title(\"Prediction+decision boundary\")\n",
    "\n",
    "ax2.scatter(x = X_train[:,0], y = X_train[:,1], c = -y_pred, cmap = 'coolwarm')\n",
    "ax2.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "ax2.set_xlim([x1min-0.5, x1max+0.5])\n",
    "ax2.set_ylim([x2min-0.5, x2max+0.5])\n",
    "\n",
    "p1 = (x1min, -(w_logistic[0] + (x1min)*w_logistic[1])/w_logistic[2])\n",
    "p2 = (x1max, -(w_logistic[0] + (x1max)*w_logistic[1])/w_logistic[2])\n",
    "\n",
    "lines = ax2.plot([p1[0], p2[0]], [p1[1], p2[1]], '-')\n",
    "plt.setp(lines, color='g', linewidth=4.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">Confusion matrix (training set)</font>\n",
    "\n",
    "Recall that the logistic regression returns a score $\\hat{p}$ in $[0,1]$, which can be interpreted as the probability $p(y=1|\\mathbf{x})$. To compute the confusion matrix, one needs to choose a threshold value $T$ to decide the final class label (that is  $\\hat{y} = 1 \\Longleftrightarrow \\hat{p} \\geq T$ ).\n",
    "\n",
    "Play with the threshold value in the code (following cell). Did you manage to find a threshold value (other than 0.5) that improves accuracy? How threshold relates to TP, FP, TN and FN ? Comment.\n",
    "\n",
    "===> Your comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y, y_pred):\n",
    "    \"\"\"\n",
    "    It receives an array with the ground-truth (y)\n",
    "    and another with the prediction (y_pred), both with binary labels\n",
    "    (positve=+1 and negative=-1) and plots the confusion\n",
    "    matrix.\n",
    "    It uses P (positive class id) and N (negative class id)\n",
    "    which are \"global\" variables ...\n",
    "    \"\"\"\n",
    "    TP = np.sum((y_pred == 1) * (y == 1))\n",
    "    TN = np.sum((y_pred == -1) * (y == -1))\n",
    "\n",
    "    FP = np.sum((y_pred == 1) * (y == -1))\n",
    "    FN = np.sum((y_pred == -1) * (y == 1))\n",
    "\n",
    "    total = TP+FP+TN+FN\n",
    "    print(\"TP = %4d    FP = %4d\\nFN = %4d    TN = %4d\"%(TP,FP,FN,TN))\n",
    "    print(\"Accuracy = %d / %d (%f)\\n\" %((TP+TN),total, (TP+TN)/total))\n",
    "    confusion = [\n",
    "        [TP/(TP+FN), FP/(TN+FP)],\n",
    "        [FN/(TP+FN), TN/(TN+FP)]\n",
    "    ]\n",
    "\n",
    "    df_cm = pd.DataFrame(confusion, \\\n",
    "                         ['$\\hat{y} = %d$'%P, '$\\hat{y} = %d$'%N],\\\n",
    "                         ['$y = %d$'%P, '$y = %d$'%N])\n",
    "    plt.figure(figsize = (8,4))\n",
    "    sb.set(font_scale=1.4)\n",
    "    sb.heatmap(df_cm, annot=True) #, annot_kws={\"size\": 16}, cmap = 'coolwarm')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "threshold = 0.5\n",
    "\n",
    "p_hat = predict_logistic(X_train, w_logistic)\n",
    "y_hat = np.where(p_hat > threshold, 1, -1)\n",
    "\n",
    "total = len(y_hat)\n",
    "\n",
    "plot_confusion_matrix(y_train, y_hat)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"background-color: #f7dc6f\">4.3.3. Testing</font>\n",
    "\n",
    "Now that you have trained the algorithm, let us evaluate its performance on the test set. Plot the scatter plot graphs (as above) and the confusion matrix (using <tt>threshold=0.5</tt>). Do you think the algorithm is doing a good generalization? Comment.</font>\n",
    "\n",
    "===> Your comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> Your code for the scatter plot (test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> your code for the confusion matrix (test set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra (optional)\n",
    "\n",
    "- If you got to this point, make a copy of your notebook.\n",
    "Run the <i>copy notebook</i> changing the number of positive and negative examples in the MNIST case. Try an unbalanced training set and observe if there are any effects in the accuracy on the test set. Additionaly, you may try with a different pair of classes. If you wish, you can summarize HERE the experiments you did and comment whatever you found interesting. There is no need to submit the <i>copy notebook</i>.</font>\n",
    "\n",
    "- You can also compare the results obtained with your algorithm with the ones generated by a standard implementation like the one in the scikit-learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===> Your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "The organization of the code in this notebook does not follow the principles of good programming. For instance, a same piece of code is used repeatedly in several places, the same variable name is used to keep data of different nature, and so on.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
